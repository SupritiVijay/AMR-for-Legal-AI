# AMR-Pretraining

Unlock the potential of pre-training AMR (Abstract Meaning Representation) models using LegalBERT with the resources provided in this repository.

## Scripts:

To initiate the pre-training of AMR models, utilize the following Jupyter notebook:

- `pre-training-legalbert.ipynb`: This [notebook](pre-training-legalbert) is your gateway to pre-training LegalBERT using AMR data.

### Instructions

Before executing the code, ensure you tailor the following statement within the notebook to the correct path:

```python
base_path = "**location_of_training_parsed_dataset**"
```

Replace `"**location_of_training_parsed_dataset**"` with the actual path where your training parsed dataset is located. This crucial step ensures a seamless and accurate pre-training process for your AMR models. Elevate your AMR pre-training journey with precision and ease.